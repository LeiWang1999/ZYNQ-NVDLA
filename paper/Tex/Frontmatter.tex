%---------------------------------------------------------------------------%
%->> Frontmatter
%---------------------------------------------------------------------------%
%-
%-> 生成封面
%-
\maketitle% 生成中文封面
% \MAKETITLE% 生成英文封面
%-
%-> 作者声明
%-
% \makedeclaration% 生成声明页
%-
%-> 中文摘要
%-
% \intobmk\chapter*{摘要}% 显示在书签但不显示在目录
% syntax: \chapter[目录]{标题}\chaptermark{页眉}
\pagestyle{noheaderstyle}% 如果需要移除整章的页眉
\chapter[摘要]{\MyTitleCh}\chaptermark{摘要}
\setcounter{page}{1}% 开始页码
\pagenumbering{Roman}% 页码符号

\begin{center}
\vspace{-0.3cm}
\zihao{3} \songti 摘要
\vspace{0.3cm}
\end{center}

近年来，深度神经网络已经被证明在包括图像分类、目标检测和自然语言处理等任务上能够取得相当不错的效果。现如今，大量的应用程序都配备了与之相关的深度学习算法，但是对于手机、无人机等资源有限的嵌入式设备、或者是时常需要处理深度神经网络作业的服务器上，仅以软件优化作为加速深度神经网络运行的手段已经不能满足日益增长的速度和能效要求，如何设计领域专用的深度神经网络加速器已经成为学术领域的研究热点。

在硬件设计层面，本设计主要基于 ZYNQ 7000 器件将英伟达开源的 NVDLA 框架在 FPGA 侧进行了实现与评估，并将其通过 AXI4 总线与双核的 ARM A9 处理器进行互联。软件设计层面，在服务器端，使用 Caffe 框架针对 MNIST、CIFAR10、IMAGENET 训练了三个有精度的分类模型，并结合 TensorRT 进行了 INT8 量化；在主机端，编译神经网络编译器接受训练好的模型生成序列化数据流文件；在硬件端，本设计为 ARM A9 处理器移植了 Ubuntu 操作系统，将硬件加速器的驱动程序挂载到 Linux 内核，通过运行时调度硬件加速器进行推理，分析了该过程中的精度损失情况、与 CPU 相比的运行速度差异，并分析了原因。

{
    \zihao{5}
    \keywords{深度神经网络 \quad FPGA \quad NVDLA \quad 硬件加速}% 中文关键词
}
%-
%-> 英文摘要
%-
% \intobmk\chapter*{Abstract}% 显示在书签但不显示在目录
\chapter[Abstract]{\MyTitleEn}\chaptermark{Abstract}

\begin{center}
\vspace{-0.3cm}
\zihao{3} \songti Abstract
\vspace{0.3cm}
\end{center}

In recent years, deep neural networks have been proven to achieve quite good results in tasks including image classification, target detection, and natural language processing. Nowadays, a large number of applications are equipped with related deep learning algorithms. However, for embedded devices with limited resources such as mobile phones and drones, or servers that often need to process deep neural network operations, only software optimization is required. As a means of accelerating deep neural networks, it can no longer meet the increasing speed and energy efficiency requirements. How to design field-specific deep neural network accelerators has become a research hotspot in the academic field.

At the hardware design level, this design is mainly based on the ZYNQ 7000 device. The Nvidia open source NVDLA framework is implemented and evaluated on the FPGA side, and it is interconnected with the dual-core ARM A9 processor through the AXI4 bus. At the software design level, on the server side, the Caffe framework is used to train three accurate classification models for MNIST, CIFAR10, and IMAGENET, and combined with TensorRT for INT8 quantization; on the host side, the neural network compiler is compiled to receive the trained model generation Serialized data stream files; on the hardware side, this design transplants the Ubuntu operating system to the ARM A9 processor, mounts the driver of the hardware accelerator to the Linux kernel, and schedules the hardware accelerator at runtime for reasoning, and analyzes the process of The accuracy loss, the difference in operating speed compared with the CPU, and the reasons are analyzed.

\KEYWORDS{Deep neural network; FPGA; NVDLA; Hardware speedup}% 英文关键词
%---------------------------------------------------------------------------%
