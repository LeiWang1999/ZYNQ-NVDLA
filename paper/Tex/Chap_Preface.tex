\chapter{绪论}\label{chap:preface}

\section{研究背景及国内外研究现状}

总的来说，深度神经网络的硬件加速系统设计的研究可分为两个阶段，一是上世纪50年代第一次人工智能热潮开始到第二次人工智能热潮结束期间，针对感知机和多层感知机等浅层神经网络研发的神经网络计算机与芯片；二是针对第三次人工智能热潮中的深度神经网络设计的深度学习处理器。

\subsection{神经网络计算机与芯片}

在第一次人工智能热潮中，D. Hebb提出了 Hebb 学习法则，在这之后不久的1951年，M. Minsky研制出了国际上首台神经网络模拟器 SNARC ；1957年，F. Rosenblatt提出了感知机模型，随后国际上首台基于感知机的神经网络计算机 Mark-I 就在1960年被研制出来，它可以连接到照相机上使用单层感知机完成简单的图像处理任务。

在第二次人工智能热潮中，著名的反向传播算法被提出使得神经网络的研究取得了一些重要突破。20世纪80年代和90年代初，很多大公司、创业公司和研究机构都参与到了神经网络计算机/芯片的研制，包括Intel公司研发的ETANN\cite{118698}、1990年发布的 CNAPS\cite{5726581} 、基于脉动阵列结构的 MANTRA I\cite{714364}，以及1997年由中国科学院半导体研究所研发的预言神\cite{人工神经网络硬件化途径与神经计算机研究}等。

然而，从当今深度学习技术发展的角度来看，这些早期的神经网络计算机/芯片有诸多缺陷，由于其只能处理很小规模的浅层神经网络算法，所以没有在工业实践中获得广泛应用。这一方面是因为浅层神经网络的应用比较局限，缺乏像当下的诸如目标检测、自然语言处理等领域的核心应用；另一方面，当时的主流集成电路工艺还是1微米工艺（今天的主流集成电路工艺已经达到7纳米），在一个芯片上只能放数量相当少的运算器，Intel的 ETANN 芯片中仅能集成64个硬件神经元；最后，受限于当时的计算机体系结构技术还没有发展成熟，没有办法将大规模的算法神经元映射到少数的硬件神经元上。

而第二次人工智能热潮也随着日本的五代机计划失败而结束。基于上述原因，从20世纪90年代开始，神经网络计算机/芯片的创业公司纷纷破产，大公司也裁减掉了相关的研究部门，各个国家暂停了这方面的科研资助，但这些瓶颈在处于第三次人工智能热潮的今天都得到了解决和改善，于是深度神经网络的硬件加速系统设计进入了第二个阶段。

\subsection{深度学习处理器}

2006年，深度学习技术由G. Hinton、Y. Bengio和Y. LeCun等人的推动而兴起，于是有了第三次人工智能热潮。而深度学习处理器也在这种环境下重新焕发了生机，在2008年，中国科学院计算技术研究所的陈云霁、陈天石等人开始了人工智能和芯片设计的交叉研究，之后来自法国Inria的O. Temam也参与到可项目中。在这些人的推动下，国际上第一个深度学习处理器架构 DianNao 于2013年被设计出来。和第一阶段设计的神经网络计算机/芯片不同， DianNao 架构不会受到网络规模的限制，可以灵活、高效地处理上百层的深度学习神经网络，并且显得更对于通用的CPU， DianNao\cite{10.1145/2654822.2541967} 可以取得两个以上数量级的能效优势。2014年，陈云霁等人在 DianNao 架构上改进，设计出了国际上首个多核的深度学习处理器架构 DaDianNao\cite{7011421} ，以及机器学习处理器架构 PuDianNao \cite{10.1145/2786763.2694358} 。进一步，中国科学院计算技术研究所提出了国际上首个深度学习指令集 Cambricon\cite{7551409} 。在这之后，首款深度学习处理器芯片“寒武纪1号”问世，目前由寒武纪研发的深度学习处理器已经应用于超过一亿台嵌入式设备中。

\section{研究意义及前景}

深度神经网络已经被证明在包括图像分类、目标检测和自然语言处理等任务上能够取得相当不错的效果。而随着其层数和神经元数量以及突触的不断增长，CPU 和 GPU 等传统体系已经很难满足神经网络增长的速度和需求。例如2016年，Google公司研发的 AlphaGo 与李世石进行围棋对弈时使用了1202个 CPU 以及176个 GPU\cite{薛永红2019机器下棋的历史与启示} ，在这场比赛中 AlphaGo 每盘棋需要消耗上千美元的电费，而作为人类的李世石一盘棋的功耗仅需要20瓦，从此可以看出传统芯片的速度和能效难以满足大规模深度学习应用的需求。如今，大量的应用程序都配备了与之相关的深度学习算法，但是仅以软件优化作为加速深度神经网络的手段已经不能满足日益增长的速度和能耗要求，基于深度神经网络的智能应用需要广泛普及的趋势使高性能、低功耗的深度学习处理器研发呼之欲出，如何利用硬件设计加速器已经成为学术领域的研究热点。

\section{研究内容及结构安排}

本设计首先参考 CNNIOT 使用 HLS 构建了一种通用的卷积神经网络硬件加速器，实现了 Lenet5 网络的加速，并使用 Bootstrap 设计了用户友好的交互页面。而在分析了 HLS 设计的硬件加速器的缺点之后，本设计重点将英伟达开源的 NVDLA 框架映射到 FPGA 开发板卡上。

硬件设计层面，本文主要在 FPGA 平台上设计实现了 NVDLA 深度学习加速器，并将其接口封装为 AXI4 总线协议与双核 ARM A9 处理器互联。软件设计层面，在前端，使用神经网络编译器接受预训练的神经网络模型，并做硬件无关优化。在后端，将驱动程序挂载到了 Ubuntu 操作系统上，由 Runtime 自动调度推理。最后，分析了所设计的硬件加速器在 FPGA 上的最高工作频率与精度损失，与 ARM A9 处理器的推理速度进行了对比。

本文共设有七个章节，各章节的内容安排如下：

第一章：主要介绍了课题的研究背景，结合国内外研究现状，提出本设计的研究意义与研究方向，并对本文的内容做基本介绍。

第二章：主要对本文涉及到的相关技术和知识背景进行介绍，首先简要介绍了深度神经网络算法的发展过程，其次对 FPGA 与 ZYNQ 架构进行了简单的概述，最后对现有的神经网络硬件加速平台及其应用进行了介绍。

第三章：根据系统设计框图介绍了系统的设计思路，包括用户友好的设计页面的演示与硬件设计架构与互联方式，对比了 CNNIOT 与 NVDLA 两个架构设计，最后简单分析了验证平台的选型。

第四章：对系统的硬件设计实现进行了详细介绍，包括了 NVDLA IP 的生成与 BlockDesign 设计两个部分，简单讲述了一下静态时序分析方法。

第五章：对系统的软件设计实现进行了详细介绍，包括结合TensorRT 进行的量化方案，为板卡移植 Ubuntu 的根文件系统，以及 Runtime 的编译。

第六章：对设计完成的 IP 进行测试与分析，主要包括基于静态时序分析得出 NVDLA 工作的最大频率，针对预训练的三个网络模型的精度损失情况以及推理速度分析。

第七章：总结研究内容，分析了本次设计的不足，以及其中可以进一步研究与改进的方向进行讨论。



