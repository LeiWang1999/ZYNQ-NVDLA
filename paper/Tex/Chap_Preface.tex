\chapter{绪论}\label{chap:preface}

\section{研究背景及国内外研究现状}

总的来说，深度神经网络的硬件加速系统设计可以分为两个阶段，一是上世纪50年代第一次人工智能热潮开始到第二次人工智能热潮结束期间，针对感知机和多层感知机等浅层神经网络研发的神经网络计算机/芯片，二是针对第三次人工智能热潮中的深度神经网络设计的深度学习处理器。

\subsection{针对感知机和多层感知机等浅层神经网络研发的神经网络计算机/芯片}

在第一次人工智能热潮中，D. Hebb提出了 Hebb 学习法则，在这之后不久的1951年，M. Minsky研制出了国际上首台神经网络模拟器 SNARC ；1957年，F. Rosenblatt提出了感知机模型，随后国际上首台基于感知机的神经网络计算机 Mark-I 就在1960年被研制出来，它可以连接到照相机上使用单层感知机完成简单的图像处理任务。

在第二次人工智能热潮中，著名的反向传播算法被提出使得神经网络的研究取得了一些重要突破。20世纪80年代和90年代初，很多大公司、创业公司和研究机构都参与到了神经网络计算机/芯片的研制，包括Intel公司研发的ETANN、1990年发布的 CNAPS 、基于脉动阵列结构的 MANTRA I ，以及1997年由中国科学院半导体研究所研发的预言神等。

然而，从当今深度学习技术发展的角度来看，这些早期的神经网络计算机/芯片有诸多缺陷，由于其只能处理很小规模的浅层神经网络算法，所以没有在工业实践中获得广泛应用。这一方面是因为浅层神经网络的应用比较局限，缺乏像当下的诸如目标检测、自然语言处理等领域的核心应用；另一方面，当时的主流集成电路工艺还是1微米工艺（今天的主流集成电路工艺已经达到7纳米），在一个芯片上只能放数量相当少的运算器，Intel的 ETANN 芯片中仅能集成64个硬件神经元；最后，受限于当时的计算机体系结构技术还没有发展成熟，没有办法将大规模的算法神经元映射到少数的硬件神经元上。

而第二次人工智能热潮也随着日本的五代机计划失败而结束。基于上述原因，从20世纪90年代开始，神经网络计算机/芯片的创业公司纷纷破产，大公司也裁减掉了相关的研究部门，各个国家暂停了这方面的科研资助，但这些瓶颈在处于第三次人工智能热潮的今天都得到了解决和改善，于是深度神经网络的硬件加速系统设计进入了第二个阶段。

\subsection{针对第三次人工智能热潮的深度学习处理器}
2006年，深度学习技术由G. Hinton、Y. Bengio和Y. LeCun等人的推动而兴起，于是有了第三次人工智能热潮。而深度学习处理器也在这种环境下重新焕发了生机，在2008年，中国科学院计算技术研究所的陈云霁、陈天石等人开始了人工智能和芯片设计的交叉研究，之后来自法国Inria的O. Temam也参与到可项目中。在这些人的推动下，国际上第一个深度学习处理器架构 DianNao 于2013年被设计出来。和第一阶段设计的神经网络计算机/芯片不同， DianNao 架构不会受到网络规模的限制，可以灵活、高效地处理上百层的深度学习神经网络，并且显得更对于通用的CPU， DianNao 可以去的两个以上数量级的能效优势。2014年，陈云霁等人在 DianNao 架构上改进，设计出了国际上首个多核的深度学习处理器架构 DaDianNao ，以及机器学习处理器架构 PuDianNao 。进一步，中国科学院计算技术研究所提出了国际上首个深度学习指令集 Cambricon 。在这之后，首款深度学习处理器芯片“寒武纪1号”问世，目前寒武纪系列处理器已经应用于超过一亿台嵌入式设备中。


\section{研究意义及前景}

深度神经网络已经被证明在包括图像分类、目标检测和自然语言处理等任务上能够取得相当不错的效果。而随着其层数和神经元数量以及突触的不断增长， CPU 和 GPU 等传统体系已经很难满足神经网络增长的速度和需求。例如2016年，Google公司研发的 AlphaGo 与李世石进行围棋对弈时使用了1202个 CPU 以及176个 GPU ，在这场比赛中 AlphaGo 每盘棋需要消耗上千美元的电费，而作为人类的李世石一盘棋的功耗仅需要20瓦，从此可以看出传统芯片的速度和能效难以满足大规模深度学习应用的需求。如今，大量的应用程序都配备了与之相关的深度学习算法，但是对于手机、无人机等资源有限的嵌入式设备上，仅用软件方式加速深度神经网络已经不能满足日益增长的速度和功耗要求，基于深度神经网络的智能应用需要广泛普及的趋势使高性能、低功耗的深度学习处理器研发呼之欲出，如何利用硬件设计加速器已经成为学术领域的研究热点。

\section{研究内容及结构安排}

硬件设计层面，本文主要在 FPGA 平台上设计实现了 NVDLA 深度学习加速器，并将其接口封装为 AXI4 总线协议与双核 ARM A9 处理器互联。软件设计层面，在前端，使用神经网络编译器接受预训练的神经网络模型，并做硬件无关优化。在后端，将驱动程序挂载到了 Ubuntu 操作系统上，由 Runtime 自动调度推理。最后，分析了所设计的硬件加速器的性能。

本文共有七个章节，论文结构安排如下：

第一章：主要介绍了课题的研究背景及研究意义，分析深度神经网络加速器系统设计的国内外研究现状，并提出本文的研究方向。

第二章：主要对本文研究所涉及到的相关技术背景的介绍，首先介绍了人工神经网络，并对本文所使用的算法——YOLO V2 算法进行了详细的分析，同时介绍了 RISC-V 处理器相关的背景以及在本系统中为何选用 RISC-V 处理器。

第三章：总体介绍了系统的设计思路：先对 YOLO V2 算法进行了详细的分析，结合 FPGA 与 CPU 各自的特点，对整个神经网络算法的加速进行了任务划分，并给出本设计所使用的逻辑结构。

第四章：对 CNN 加速器进行详细设计及优化，介绍了使用 HLS 设计 CNN 加速器过程中的优化方案以及具体应用情况。

第五章：由于 CNN 加速器需要配合 RISC-V 软核进行异构运算，因此本章将 CNN 加速器作为 RISC-V 处理器的外设，设计了一个通用 RISC-V SOC，介绍了该 SOC 的硬件架构以及详细设计过程。

第六章：主要计算分析了 CNN 加速器的资源消耗，结合常见的运算平台，设计了3组典型的对照环境，将本设计中的异构计算系统与其分别进行对比，分析了该异构计算系统的优缺点。

第七章：总结全文的研究内容，对本课题中可以进一步研究的方向进行讨论。



