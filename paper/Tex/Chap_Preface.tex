\chapter{绪论}\label{chap:preface}

\section{研究背景及国内外研究现状}

总的来说，深度神经网络的硬件加速系统设计可以分为两个阶段，一是上世纪50年代第一次人工智能热潮开始到第二次人工智能热潮结束期间，针对感知机和多层感知机等浅层神经网络研发的神经网络计算机/芯片，二是针对第三次人工智能热潮中的深度神经网络设计的深度学习处理器。

\subsection{针对感知机和多层感知机等浅层神经网络研发的神经网络计算机/芯片}

在第一次人工智能热潮中，D. Hebb提出了 Hebb 学习法则，在这之后不久的1951年，M. Minsky研制出了国际上首台神经网络模拟器 SNARC ；1957年，F. Rosenblatt提出了感知机模型，随后国际上首台基于感知机的神经网络计算机 Mark-I 就在1960年被研制出来，它可以连接到照相机上使用单层感知机完成简单的图像处理任务。

在第二次人工智能热潮中，著名的反向传播算法被提出使得神经网络的研究取得了一些重要突破。20世纪80年代和90年代初，很多大公司、创业公司和研究机构都参与到了神经网络计算机/芯片的研制，包括Intel公司研发的ETANN\cite{118698}、1990年发布的 CNAPS\cite{5726581} 、基于脉动阵列结构的 MANTRA I\cite{714364}，以及1997年由中国科学院半导体研究所研发的预言神\cite{人工神经网络硬件化途径与神经计算机研究}等。

然而，从当今深度学习技术发展的角度来看，这些早期的神经网络计算机/芯片有诸多缺陷，由于其只能处理很小规模的浅层神经网络算法，所以没有在工业实践中获得广泛应用。这一方面是因为浅层神经网络的应用比较局限，缺乏像当下的诸如目标检测、自然语言处理等领域的核心应用；另一方面，当时的主流集成电路工艺还是1微米工艺（今天的主流集成电路工艺已经达到7纳米），在一个芯片上只能放数量相当少的运算器，Intel的 ETANN 芯片中仅能集成64个硬件神经元；最后，受限于当时的计算机体系结构技术还没有发展成熟，没有办法将大规模的算法神经元映射到少数的硬件神经元上。

而第二次人工智能热潮也随着日本的五代机计划失败而结束。基于上述原因，从20世纪90年代开始，神经网络计算机/芯片的创业公司纷纷破产，大公司也裁减掉了相关的研究部门，各个国家暂停了这方面的科研资助，但这些瓶颈在处于第三次人工智能热潮的今天都得到了解决和改善，于是深度神经网络的硬件加速系统设计进入了第二个阶段。

\subsection{针对第三次人工智能热潮的深度学习处理器}
2006年，深度学习技术由G. Hinton、Y. Bengio和Y. LeCun等人的推动而兴起，于是有了第三次人工智能热潮。而深度学习处理器也在这种环境下重新焕发了生机，在2008年，中国科学院计算技术研究所的陈云霁、陈天石等人开始了人工智能和芯片设计的交叉研究，之后来自法国Inria的O. Temam也参与到可项目中。在这些人的推动下，国际上第一个深度学习处理器架构 DianNao 于2013年被设计出来。和第一阶段设计的神经网络计算机/芯片不同， DianNao 架构不会受到网络规模的限制，可以灵活、高效地处理上百层的深度学习神经网络，并且显得更对于通用的CPU， DianNao\cite{10.1145/2654822.2541967} 可以取得两个以上数量级的能效优势。2014年，陈云霁等人在 DianNao 架构上改进，设计出了国际上首个多核的深度学习处理器架构 DaDianNao\cite{7011421} ，以及机器学习处理器架构 PuDianNao \cite{10.1145/2786763.2694358} 。进一步，中国科学院计算技术研究所提出了国际上首个深度学习指令集 Cambricon\cite{7551409} 。在这之后，首款深度学习处理器芯片“寒武纪1号”问世，目前寒武纪系列处理器已经应用于超过一亿台嵌入式设备中。

\section{研究意义及前景}

深度神经网络已经被证明在包括图像分类、目标检测和自然语言处理等任务上能够取得相当不错的效果。而随着其层数和神经元数量以及突触的不断增长， CPU 和 GPU 等传统体系已经很难满足神经网络增长的速度和需求。例如2016年，Google公司研发的 AlphaGo 与李世石进行围棋对弈时使用了1202个 CPU 以及176个 GPU ，在这场比赛中 AlphaGo 每盘棋需要消耗上千美元的电费，而作为人类的李世石一盘棋的功耗仅需要20瓦，从此可以看出传统芯片的速度和能效难以满足大规模深度学习应用的需求。如今，大量的应用程序都配备了与之相关的深度学习算法，但是对于手机、无人机等资源有限的嵌入式设备上，仅用软件方式加速深度神经网络已经不能满足日益增长的速度和功耗要求，基于深度神经网络的智能应用需要广泛普及的趋势使高性能、低功耗的深度学习处理器研发呼之欲出，如何利用硬件设计加速器已经成为学术领域的研究热点。

\section{研究内容及结构安排}

硬件设计层面，本文主要在 FPGA 平台上设计实现了 NVDLA 深度学习加速器，并将其接口封装为 AXI4 总线协议与双核 ARM A9 处理器互联。软件设计层面，在前端，使用神经网络编译器接受预训练的神经网络模型，并做硬件无关优化。在后端，将驱动程序挂载到了 Ubuntu 操作系统上，由 Runtime 自动调度推理。最后，分析了所设计的硬件加速器的性能。

本文共有七个章节，论文结构安排如下：

第一章：

第二章：

第三章：

第四章：

第五章：

第六章：

第七章：



